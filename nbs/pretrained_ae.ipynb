{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders with feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python datasets[vision] keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, Reshape, Input, InputLayer\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import (load_img, img_to_array)\n",
    "from tensorflow.keras.applications.vgg16 import (VGG16, preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img, model):\n",
    "    \"\"\"\n",
    "    Extract features from image data using pretrained model (e.g. VGG16)\n",
    "\n",
    "    Arguments:\n",
    "    - img_path: path to image to extract features on\n",
    "    - model: model to use as a feature extractor\n",
    "\n",
    "    Returns:\n",
    "    - A list of image embeddings for the input image\n",
    "\n",
    "    Source:\n",
    "        This function is taken from the Session 10 Notebook for the Visual Analytics Course.\n",
    "    \"\"\"\n",
    "    # Define input image shape - remember we need to reshape\n",
    "    #input_shape = (224, 224, 3)\n",
    "    # load image from file path\n",
    "    #img = load_img(img, target_size=(input_shape[0],\n",
    "                                          #input_shape[1]))\n",
    "    # convert to array\n",
    "    img_array = img_to_array(img)\n",
    "    # expand to fit dimensions\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "    # preprocess image - see last week's notebook\n",
    "    preprocessed_img = preprocess_input(expanded_img_array)\n",
    "    # use the predict function to create feature representation\n",
    "    features = model.predict(preprocessed_img)\n",
    "    # flatten\n",
    "    flattened_features = features.flatten()\n",
    "    # normalise features\n",
    "    normalized_features = flattened_features / norm(features)\n",
    "\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  model = VGG16(weights='imagenet', \n",
    "              include_top=False,\n",
    "              pooling='avg',\n",
    "              input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_resized = ds.map(_resize_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "\n",
    "for i in range(len(ds_resized)):\n",
    "    im = ds_resized[i]['image']\n",
    "    feature = extract_features(im, model)\n",
    "    feature_list.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.asarray(feature_list)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pretrained_autoencoder(features_shape, code_size, img_shape):\n",
    "    # The encoder\n",
    "    encoder = Sequential()\n",
    "    encoder.add(InputLayer(features_shape))\n",
    "    encoder.add(Flatten()) # flatten the tensor to 1D\n",
    "    encoder.add(Dense(code_size)) # size of the final encoded vector (dense just means fully connected)\n",
    "\n",
    "    # The decoder\n",
    "    decoder = Sequential()\n",
    "    decoder.add(InputLayer((code_size,)))\n",
    "    decoder.add(Dense(np.prod(features_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072 (returns the product)\n",
    "    decoder.add(Reshape(img_shape))\n",
    "\n",
    "    return encoder, decoder\n",
    "\n",
    "def pretrained_autoencoder():\n",
    "    #n_layers = 4\n",
    "    input_img = tf.keras.layers.Input(shape=(512,), name='input')\n",
    "\n",
    "    enc1 = tf.keras.layers.Dense(200, activation = 'relu', name='encoder1')(input_img)\n",
    "    enc2 = tf.keras.layers.Dense(100, activation = 'relu', name='encoder2')(enc1)\n",
    "    \n",
    "    embedding = tf.keras.layers.Dense(30, activation = 'relu', name='encoder3')(enc2)\n",
    "    \n",
    "    dec1 = tf.keras.layers.Dense(100, activation = 'relu', name='decoder1')(embedding)\n",
    "    dec2 = tf.keras.layers.Dense(200, activation = 'relu', name='decoder2')(dec1)\n",
    "\n",
    "    decoded = tf.keras.layers.Dense(512, activation = 'relu', name='decoder3')(dec2)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=input_img, outputs=decoded, name='AE'), tf.keras.models.Model(inputs=input_img, outputs=embedding, name='encoder')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = pretrained_autoencoder()\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(features, features, batch_size=256, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_shape = features.shape[1]\n",
    "im_shape = preprocessed_ds[0]['image'].shape\n",
    "encoder, decoder = build_pretrained_autoencoder(feat_shape, 200, im_shape)\n",
    "\n",
    "inp = Input(feat_shape)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "\n",
    "autoencoder = Model(inp, reconstruction)\n",
    "autoencoder.compile(optimizer='adamax', loss='mse')\n",
    "\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi definerer vores inputs undervejs, fordi vi ikke har sagt at det er en sequential model (hvor input ellers automatisk tages fra laget over)\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    input_img = tf.keras.layers.Input(shape=(dims[0],), name='input') # shape er en 1D vector som er længden af output af densenet\n",
    "    x = input_img\n",
    "    # internal layers of the encoder\n",
    "    for i in range(n_stacks-1): # specificerer antallet af lag baseret på n_stacks variablen\n",
    "        # så her, 3 lag\n",
    "        # tror koden laver de tre lag (går dybere). det sidste (x) betyder, at det forrige x tages som input\n",
    "        x = tf.keras.layers.Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # bottleneck\n",
    "    # koden tager den sidste værdi i dims-vectoren (10) som længden på vektoren --> tager x som input\n",
    "    encoded = tf.keras.layers.Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
    "\n",
    "    x = encoded # definerer ny input variabel\n",
    "    # hidden layers of the decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = tf.keras.layers.Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # output\n",
    "    x = tf.keras.layers.Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    decoded = x\n",
    "    return tf.keras.models.Model(inputs=input_img, outputs=decoded, name='AE'), tf.keras.models.Model(inputs=input_img, outputs=encoded, name='encoder')\n",
    "\n",
    "dims = [1024, 500, 500, 2000, 10] # forstår ikke hvad formålet med den her er "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
